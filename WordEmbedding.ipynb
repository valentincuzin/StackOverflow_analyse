{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.downloader import load \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from networkx import draw_networkx, from_numpy_array, write_graphml\n",
    "from matplotlib.pyplot import get_cmap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_questions: DataFrame = read_csv('./data/Questions.csv', encoding='latin-1')\n",
    "raw_answers: DataFrame = read_csv('./data/Answers.csv', encoding='latin-1')\n",
    "raw_tags: DataFrame = read_csv('./data/Tags.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answers = raw_answers\n",
    "raw_answers = raw_answers.where(raw_answers['Score'] >= 10).dropna()\n",
    "answers_txt = raw_answers['Body'].values\n",
    "\n",
    "raw_questions = raw_questions.where(raw_questions['Score'] >= 10).dropna()\n",
    "tag_count = raw_tags['Tag'].value_counts()\n",
    "best_tags = raw_tags.join(tag_count, on='Tag').dropna()\n",
    "best_tags = best_tags.loc[best_tags.groupby('Id')['count'].idxmax()]\n",
    "\n",
    "questions = raw_questions.merge(best_tags[['Id', 'Tag']], on='Id', how='left')\n",
    "questions = questions.dropna()\n",
    "questions_txt = questions['Body'].values\n",
    "questions_title = questions[['Title', 'Score', 'Tag']].sort_values(by='Score', ascending=False)\n",
    "print(questions_title.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory:\n",
    "del raw_questions\n",
    "del raw_answers\n",
    "del questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_body_content(texts):\n",
    "\n",
    "\tdef replace_links(match): # remove href link\n",
    "\t\tif match.group(1):\n",
    "\t\t\treturn match.group(1)\n",
    "\t\telse:\n",
    "\t\t\treturn ' '\n",
    "\n",
    "\tclean_text = []\n",
    "\tfor i, text in enumerate(texts):\n",
    "\t\ttext = re.sub(r'\\n', ' ', text)\n",
    "\t\ttext = re.sub(r'<a[^>]*>(.*?)</a>', replace_links, text)\n",
    "\t\ttext = re.sub(r'<[^>]+>', ' ', text)\n",
    "\t\tclean_text.append(text)\n",
    "\treturn clean_text\n",
    "\n",
    "answers_txt = clean_body_content(answers_txt)\n",
    "questions_txt = clean_body_content(questions_txt)\n",
    "\n",
    "full_doc_to_clean = []\n",
    "full_doc_to_clean.extend(answers_txt)\n",
    "full_doc_to_clean.extend(questions_title['Title'])\n",
    "full_doc_to_clean.extend(questions_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(full_doc_to_clean)\n",
    "tfidf_dict = {}\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    tfidf_dict[ele1] = ele2\n",
    "tfidf_res = Series(tfidf_dict)\n",
    "print(tfidf_res.describe())\n",
    "tfidf_res = tfidf_res.sort_values()\n",
    "useless_word = tfidf_res.where(tfidf_res >= tfidf_res.mean()).dropna()\n",
    "useless_word.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'^\\d+$') # only number\n",
    "def clean_stop_words(text: str) -> str:\n",
    "\twords = text.split(' ')\n",
    "\twords = [word for word in words if (word.lower() not in ENGLISH_STOP_WORDS \n",
    "\t\t\t\t\t\t\t\t\t    and word.lower() not in useless_word.index\n",
    "\t\t\t\t\t\t\t\t\t\tand not pattern.match(word))]\n",
    "\treturn \" \".join(words)\n",
    "full_doc_clean = [clean_stop_words(text) for text in full_doc_to_clean]\n",
    "questions_title['Title'] = questions_title['Title'].apply(clean_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "\tdef __iter__(self):\n",
    "\t\tfor line in full_doc_to_clean:\n",
    "\t\t\tyield simple_preprocess(line)\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model_raw = Word2Vec(sentences, sg=1, epochs=10, workers=8) # take long time\n",
    "model_raw.save(\"data/word2vec_raw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "\tdef __iter__(self):\n",
    "\t\tfor line in full_doc_clean:\n",
    "\t\t\tyield simple_preprocess(line)\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = Word2Vec(sentences, sg=1, epochs=10, workers=8) # take long time\n",
    "model.save(\"data/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = load('word2vec-google-news-300') # take long time\n",
    "model = Word2Vec.load('./data/word2vec.model')\n",
    "model_raw = Word2Vec.load('./data/word2vec_raw.model')\n",
    "print(pretrain_model.most_similar(positive=['java'], topn=5))\n",
    "print(model.wv.most_similar(positive=['java'], topn=5))\n",
    "print(model_raw.wv.most_similar(positive=['java'], topn=5))\n",
    "del pretrain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette comparaison démontre l'importance de ne pas utiliser un modèle pré-entrainé, car le sens de mot ne sera pas saisit de la même façon qu'avec notre dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2vec:\n",
    "\tdef __init__(self, path_to_model, tfidf_dict):\n",
    "\t\tself.word2vec = Word2Vec.load(path_to_model)\n",
    "\t\tself.tfidf_dict: dict = tfidf_dict\n",
    "\t\n",
    "\tdef encode(self, sentence):\n",
    "\t\tvec = np.zeros((1, self.word2vec.vector_size))\n",
    "\t\tfor word in sentence.split(\" \"):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif word.lower() in self.tfidf_dict.keys():\n",
    "\t\t\t\t\tvec += self.tfidf_dict[word.lower()] * self.word2vec.wv[word.lower()]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tvec += self.word2vec.wv[word.lower()]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\treturn vec\n",
    "\t\n",
    "\t\n",
    "\tdef similarity(self, sentence1, sentence2):\n",
    "\t\tvec1 = self.encode(sentence1)\n",
    "\t\tvec2 = self.encode(sentence2)\n",
    "\t\treturn cosine_similarity(vec1, vec2).item()\n",
    "\t\n",
    "\tdef series_similarity(self, sentences):\n",
    "\t\tres = dict()\n",
    "\t\tfor i in range(len(sentences)):\n",
    "\t\t\tif i%100==0:\n",
    "\t\t\t\tprint(i) \n",
    "\t\t\tfor j in range(i, len(sentences)):\n",
    "\t\t\t\tif i != j:\n",
    "\t\t\t\t\tsimilarity = self.similarity(sentences[i], sentences[j])\n",
    "\t\t\t\t\tres[(i, j)] = similarity\n",
    "\t\treturn Series(res)\n",
    "\t\n",
    "\tdef matrix_similarity(self, sentences, tags, scores):\n",
    "\t\tres = np.empty(shape=(len(sentences), len(sentences)))\n",
    "\t\tattr = []\n",
    "\t\tfor i in range(len(sentences)):\n",
    "\t\t\tif i%100==0:\n",
    "\t\t\t\tprint(i)\n",
    "\t\t\tattr.append({\"Title\": sentences[i],\n",
    "\t\t\t\t\t\t\"Tag\": tags[i],\n",
    "\t\t\t\t\t\t\"Score\": scores[i]})\n",
    "\t\t\tfor j in range(i, len(sentences)):\n",
    "\t\t\t\tif i != j:\n",
    "\t\t\t\t\tsimilarity = self.similarity(sentences[i], sentences[j])\n",
    "\t\t\t\t\tres[i, j] = similarity\n",
    "\t\t\t\t\tres[j, i] = similarity\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tres[i, j] = 0\n",
    "\t\t\n",
    "\t\treturn res, attr\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(questions_title['Title'].values)\n",
    "tfidf_dict = {}\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    tfidf_dict[ele1] = ele2\n",
    "\n",
    "doc_model = Doc2vec('./data/word2vec.model', tfidf_dict)\n",
    "doc_model_raw = Doc2vec('./data/word2vec_raw.model', tfidf_dict)\n",
    "doc_model_visu = Doc2vec('./data/word2vec_visu.model', tfidf_dict)\n",
    "doc_model.encode(questions_title['Title'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_title.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions_title['Title'].loc[188], '/', questions_title['Title'].loc[1483], \n",
    "doc_model_raw.similarity(questions_title['Title'].loc[188], questions_title['Title'].loc[1483]))\n",
    "print()\n",
    "print(questions_title['Title'].loc[999], '/', questions_title['Title'].loc[1060], \n",
    "doc_model_raw.similarity(questions_title['Title'].loc[999], questions_title['Title'].loc[1060]))\n",
    "print()\n",
    "print(questions_title['Title'].loc[1143], '/', questions_title['Title'].loc[236], \n",
    "doc_model_raw.similarity(questions_title['Title'].loc[1143], questions_title['Title'].loc[236]))\n",
    "print()\n",
    "print(questions_title['Title'].loc[1188], '/', questions_title['Title'].loc[733], \n",
    "doc_model_raw.similarity(questions_title['Title'].loc[1188], questions_title['Title'].loc[733]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions_title['Title'].loc[188], '/', questions_title['Title'].loc[1483], \n",
    "doc_model.similarity(questions_title['Title'].loc[188], questions_title['Title'].loc[1483]))\n",
    "print()\n",
    "print(questions_title['Title'].loc[999], '/', questions_title['Title'].loc[1060], \n",
    "doc_model.similarity(questions_title['Title'].loc[999], questions_title['Title'].loc[1060]))\n",
    "print()\n",
    "print(questions_title['Title'].loc[1143], '/', questions_title['Title'].loc[236], \n",
    "doc_model.similarity(questions_title['Title'].loc[1143], questions_title['Title'].loc[236]))\n",
    "print()\n",
    "print(questions_title['Title'].loc[1188], '/', questions_title['Title'].loc[733], \n",
    "doc_model.similarity(questions_title['Title'].loc[1188], questions_title['Title'].loc[733]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_raw = doc_model_raw.series_similarity(questions_title['Title'].values[:500])\n",
    "res_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = doc_model.series_similarity(questions_title['Title'].values[:500])\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./data/word2vec_visu.model')\n",
    "\n",
    "total_encode = []\n",
    "final_tag = ['ia', 'python', 'pandas', 'notebook', 'pip', 'pypi']\n",
    "\n",
    "for tag in final_tag:\n",
    "\ttry:\n",
    "\t\ttotal_encode.append(model.wv[tag])\n",
    "\texcept Exception as e:\n",
    "\t\tprint(str(e))\t\t\n",
    "total_encode = DataFrame(total_encode)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver([0 for _ in range(len(total_encode))], [0 for _ in range(len(total_encode))], total_encode[0], total_encode[1], angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "# Ajouter les labels\n",
    "for i, tag in enumerate(final_tag):\n",
    "    ax.annotate(tag, (total_encode[0][i], total_encode[1][i]))\n",
    "# Définir les limites du plot pour inclure toutes les flèches\n",
    "ax.set_xlim([min(total_encode[0]) - 1, max(total_encode[0]) + 1])\n",
    "ax.set_ylim([min(total_encode[1]) - 10, max(total_encode[1]) + 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_encode = []\n",
    "final_tag = ['linux', 'windows', 'mac', 'ubuntu', 'powershell', 'bash', 'shell', 'bug']\n",
    "\n",
    "for tag in final_tag:\n",
    "\ttry:\n",
    "\t\ttotal_encode.append(model.wv[tag])\n",
    "\texcept Exception as e:\n",
    "\t\tprint(str(e))\t\t\n",
    "total_encode = DataFrame(total_encode)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver([0 for _ in range(len(total_encode))], [0 for _ in range(len(total_encode))], total_encode[0], total_encode[1], angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "# Ajouter les labels\n",
    "for i, tag in enumerate(final_tag):\n",
    "    ax.annotate(tag, (total_encode[0][i], total_encode[1][i]))\n",
    "# Définir les limites du plot pour inclure toutes les flèches\n",
    "ax.set_xlim([min(total_encode[0]) - 1, max(total_encode[0]) + 1])\n",
    "ax.set_ylim([min(total_encode[1]) - 5, max(total_encode[1]) + 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_encode = []\n",
    "for question in questions_title['Title'].values:\n",
    "\ttotal_encode.append(doc_model_visu.encode(question)[0])\n",
    "total_encode = DataFrame(total_encode)\n",
    "total_encode\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(total_encode)\n",
    "encode_norm = scaler.transform(total_encode)\n",
    "encode_norm = DataFrame(encode_norm)\n",
    "\n",
    "# colorise by tag\n",
    "cmap = get_cmap('hsv')\n",
    "tags = set()\n",
    "c_tag = {}\n",
    "for tag in questions_title['Tag'].values:\n",
    "\ttags.add(tag)\n",
    "for i, tag in enumerate(tags):\n",
    "\tc_tag[tag] = cmap(i)\n",
    "color = []\n",
    "for tag in questions_title['Tag'].values:\n",
    "\tcolor.append(c_tag[tag])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(encode_norm[0], encode_norm[1], c=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, attr = doc_model.matrix_similarity(questions_title['Title'].values[:500], questions_title['Tag'].values[:500], questions_title['Score'].values[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_adj = np.where(matrix > 0.7, 1, 0)\n",
    "G = from_numpy_array(matrix_adj)\n",
    "draw_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_node(G, min_degree):\n",
    "\tremove_list = []\n",
    "\tfor node in G.nodes:\n",
    "\t\tif G.degree(node) < min_degree:\n",
    "\t\t\tremove_list.append(node)\n",
    "\tG_local = G.copy()\n",
    "\tG_local.remove_nodes_from(remove_list)\n",
    "\treturn G_local\n",
    "\n",
    "G_clean1 = remove_node(G, 1)\n",
    "\n",
    "cmap = get_cmap('hsv')\n",
    "tags = set()\n",
    "c_tag = {}\n",
    "for at in attr:\n",
    "\ttags.add(at['Tag'])\n",
    "for i, tag in enumerate(tags):\n",
    "\tc_tag[tag] = cmap(i)\n",
    "color = []\n",
    "for node in G_clean1.nodes:\n",
    "\tG_clean1.nodes[node][\"tag\"] = attr[node]['Tag']\n",
    "\tG_clean1.nodes[node][\"score\"] = attr[node]['Score']\n",
    "\tG_clean1.nodes[node][\"title\"] = attr[node]['Title']\n",
    "\tcolor.append(c_tag[attr[node][\"Tag\"]])\n",
    "draw_networkx(G_clean1, with_labels=False, node_color=color)\n",
    "write_graphml(G_clean1, './data/word2vec_graph.graphml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
