{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array, zeros\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import networkx as nx\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement, clean et traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and fitting\n",
    "# retrive from here https://www.kaggle.com/datasets/stackoverflow/stacksample/data\n",
    "raw_answers: DataFrame = read_csv('./data/Answers.csv', encoding='latin-1')\n",
    "raw_questions: DataFrame = read_csv('./data/Questions.csv', encoding='latin-1')\n",
    "raw_tags: DataFrame = read_csv('./data/Tags.csv', encoding='latin-1')\n",
    "questions = raw_questions.copy()\n",
    "answers = raw_answers.copy()\n",
    "tags = raw_tags.copy()\n",
    "tags['Tag'] = tags['Tag'].astype(str)\n",
    "grouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n",
    "grouped_tags.reset_index()\n",
    "tags = tags.groupby('Id')['Tag'].apply(list).reset_index()\n",
    "questions = questions.join(tags['Tag'], on='Id')\n",
    "\n",
    "# Keep only top 10 tags\n",
    "top_tags = tags['Tag'].explode().value_counts().nlargest(10).index\n",
    "\n",
    "# Drop NaN values\n",
    "questions_top = questions.dropna(subset=['Tag'])\n",
    "\n",
    "\n",
    "# Remove rows with tags not in top_tags\n",
    "questions_top = questions_top[questions_top['Tag'].apply(lambda tags: any(tag in top_tags for tag in tags))]\n",
    "questions_top.reset_index()\n",
    "questions_top['CreationDate'] = pd.to_datetime(questions_top['CreationDate'])\n",
    "questions_top['CreationDate'] = questions_top['CreationDate'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patern frequent itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute patern frequent, and association rules\n",
    "\n",
    "# Create a list of lists of tags for each question\n",
    "# trans questions['Tag'] to list of string\n",
    "freq_pat = questions['Tag'].dropna().tolist()\n",
    "# print(freq_pat)\n",
    "# freq_pat = [(str(element) for element in list_) for list_ in freq_pat]\n",
    "\n",
    "\n",
    "# Step 1: Convert the dataset into a one-hot encoded DataFrame\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(freq_pat).transform(freq_pat)\n",
    "df = pd.DataFrame(te_data, columns=te.columns_)\n",
    "\n",
    "# Step 2: Apply the Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.001, use_colnames=True)\n",
    "\n",
    "frequent_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "print(frequent_itemsets)\n",
    "\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=0.2)\n",
    "\n",
    "rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.clear()\n",
    "threshold_support = frequent_itemsets['support'].quantile(0)\n",
    "for index, row in frequent_itemsets.iterrows():\n",
    "    if row['support'] >= threshold_support:\n",
    "        itemset = tuple(row['itemsets'])\n",
    "        G.add_node(itemset, support=row['support'])\n",
    "\n",
    "for index, row in rules.iterrows():\n",
    "    antecedent = tuple(row['antecedents'])\n",
    "    consequent = tuple(row['consequents'])\n",
    "    if G.has_node(antecedent) and G.has_node(consequent):\n",
    "        G.add_edge(antecedent, consequent, weight=row['confidence'], confidence=row['confidence'], lift=row['lift'], leverage=row['leverage'], conviction=row['conviction'], zhang=row['zhangs_metric'])\n",
    "        # G.add_edge(antecedent, consequent, weight=row['confidence_norm'], confidence=row['confidence'])\n",
    "\n",
    "node_sizes = [10000 * G.nodes[node]['support'] for node in G.nodes]\n",
    "edge_weights = [G[u][v]['weight'] * 3 for u, v in G.edges]\n",
    "\n",
    "plt.close()\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, k=0.5, seed=42)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"skyblue\", alpha=0.7)\n",
    "nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.5, edge_color=\"gray\")\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_family=\"sans-serif\")\n",
    "\n",
    "plt.title(\"Graphe des tags avec support et lift\")\n",
    "plt.savefig('Graph_association_rules2.png')\n",
    "\n",
    "nx.write_gexf(G, 'graph.gexf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other way to plot the graph\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.clear()\n",
    "\n",
    "list_confidence = rules['confidence'].tolist()\n",
    "# centrer et normaliser\n",
    "list_confidence = (list_confidence - np.mean(list_confidence)) / np.std(list_confidence)\n",
    "rules['confidence_norm'] = list_confidence\n",
    "\n",
    "# Seuil pour les nœuds\n",
    "threshold_support = frequent_itemsets['support'].quantile(0)\n",
    "for index, row in frequent_itemsets.iterrows():\n",
    "    if row['support'] >= threshold_support:\n",
    "        itemset = tuple(row['itemsets'])\n",
    "        G.add_node(itemset, support=row['support'])\n",
    "\n",
    "# Ajout des arêtes avec vérification de réciprocité\n",
    "for index, row in rules.iterrows():\n",
    "    antecedent = tuple(row['antecedents'])\n",
    "    consequent = tuple(row['consequents'])\n",
    "    if G.has_node(antecedent) and G.has_node(consequent):\n",
    "        G.add_edge(antecedent, consequent, weight=row['confidence_norm'], confidence=row['confidence'])\n",
    "\n",
    "# Calcul des tailles de nœud et des poids d'arêtes\n",
    "node_sizes = [10000 * G.nodes[node]['support'] for node in G.nodes]\n",
    "\n",
    "# Tracé des nœuds\n",
    "plt.close()\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, k=0.5, seed=42)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"skyblue\", alpha=0.7)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_family=\"sans-serif\")\n",
    "\n",
    "eppaiseur = 3\n",
    "\n",
    "# Tracé des arêtes avec vérification de réciprocité\n",
    "for u, v in G.edges():\n",
    "    if G.has_edge(v, u):  # Si une arête inverse existe\n",
    "        # Tracer le lien réciproque avec une couleur et courbure différentes\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos,\n",
    "            edgelist=[(u, v)],\n",
    "            width = eppaiseur * G[u][v]['weight'],  # Ajustement de largeur\n",
    "            alpha=0.7,\n",
    "            edge_color=\"red\",  # Couleur différente pour le lien réciproque\n",
    "            connectionstyle=\"arc3,rad=0.2\"  # Courbure\n",
    "        )\n",
    "    else:\n",
    "        # Tracer les arêtes non réciproques normalement\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos,\n",
    "            edgelist=[(u, v)],\n",
    "            width= eppaiseur * G[u][v]['weight'],\n",
    "            alpha=0.5,\n",
    "            edge_color=\"gray\"\n",
    "        )\n",
    "\n",
    "plt.title(\"Graphe des tags avec support et lift\")\n",
    "plt.savefig('Graph_association_rules2.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph = [sub for sub in nx.strongly_connected_components(G) if len(sub) >= 2]\n",
    "\n",
    "num_subgraphs = len(sub_graph)\n",
    "cols = 3\n",
    "rows = math.ceil(num_subgraphs / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Draw sub-graphs\n",
    "for i, sub in enumerate(sub_graph):\n",
    "    G_sub = G.subgraph(sub)\n",
    "    nx.write_gexf(G_sub, f'graph_sub_{i}.gexf')\n",
    "\n",
    "    node_sizes = [10000 * G_sub.nodes[node]['support'] for node in G_sub.nodes]\n",
    "    edge_weights = [G_sub[u][v]['weight'] * 1.5 for u, v in G_sub.edges]\n",
    "\n",
    "    pos = nx.spring_layout(G_sub, k=0.5, seed=42)\n",
    "    ax = axes[i]\n",
    "    nx.draw_networkx_nodes(G_sub, pos, ax=ax, node_size=node_sizes, node_color=\"skyblue\", alpha=0.7)\n",
    "    nx.draw_networkx_edges(G_sub, pos, ax=ax, width=edge_weights, alpha=0.5, edge_color=\"gray\")\n",
    "    nx.draw_networkx_labels(G_sub, pos, ax=ax, font_size=10, font_family=\"sans-serif\")\n",
    "    ax.set_title(f\"Sub-graph {i + 1} (|V|={len(G_sub.nodes)})\")\n",
    "    ax.axis('off')\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining 3.11",
   "language": "python",
   "name": "datamaning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
